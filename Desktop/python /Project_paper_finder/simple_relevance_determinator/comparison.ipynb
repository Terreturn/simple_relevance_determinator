{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84dbc976",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import platform\n",
    "import pytrec_eval\n",
    "\n",
    "# Keyword list for evaluating search engine capabilities\n",
    "# Compatible with Google Scholar and AstA Paper Finder\n",
    "# includes terms from numerical methods, econometrics, optimization, and dynamic systems\n",
    "\n",
    "keywords = [\n",
    "    \"numerical methods\",\n",
    "    \"finite element method\",\n",
    "    \"stochastic processes\",\n",
    "    \"optimization theory\",\n",
    "    \"statistical learning\",\n",
    "    \"panel data estimation\",\n",
    "    \"instrumental variables estimation\",\n",
    "    \"causal inference econometrics\",\n",
    "    \"nonparametric econometric methods\",\n",
    "    \"dynamic optimization in economics\",\n",
    "    \"general equilibrium theory\",\n",
    "    \"dynamic stochastic general equilibrium models\",\n",
    "    \"Hamilton–Jacobi–Bellman equation economics\",\n",
    "    \"viscosity solutions economic models\",\n",
    "    \"nonlinear dynamical systems in economics\",\n",
    "    \"Monte Carlo methods econometrics\",\n",
    "    \"numerical solution of DSGE models\",\n",
    "    \"semiparametric efficiency bound\"\n",
    "]\n",
    "\n",
    "############### apply "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26bfe7f",
   "metadata": {},
   "source": [
    "# Google scholar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b947a61d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-search-results in /opt/anaconda3/envs/trec/lib/python3.9/site-packages (2.4.2)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/trec/lib/python3.9/site-packages (from google-search-results) (2.32.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/envs/trec/lib/python3.9/site-packages (from requests->google-search-results) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/trec/lib/python3.9/site-packages (from requests->google-search-results) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/trec/lib/python3.9/site-packages (from requests->google-search-results) (2.6.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/trec/lib/python3.9/site-packages (from requests->google-search-results) (2025.11.12)\n"
     ]
    }
   ],
   "source": [
    "!pip install google-search-results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b990bb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Google Scholar search -> pytrec_eval-ready JSON\n",
    "via SerpApi\n",
    "\n",
    "Prereqs:\n",
    "  pip install google-search-results requests\n",
    "\n",
    "Env:\n",
    "  export SERPAPI_API_KEY=\"YOUR_KEY\"\n",
    "\n",
    "Outputs:\n",
    "  - run.json   (pytrec_eval run: qid -> docid -> score)\n",
    "  - qrels_template.json (empty template to fill relevance judgments)\n",
    "  - docs.json  (doc metadata by docid)\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import hashlib\n",
    "from typing import Dict, Any, List\n",
    "from serpapi import GoogleSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00ecf061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -----------------------\n",
    "# 2) Config\n",
    "# -----------------------\n",
    "import os\n",
    "os.environ[\"SERPAPI_API_KEY\"] = \"04b8e4f44d6cbb9fce4af8f818407abf71876911cc0a52074b4a6dc65221063f\"\n",
    "\n",
    "API_KEY = os.getenv(\"SERPAPI_API_KEY\")\n",
    "if not API_KEY:\n",
    "    raise RuntimeError(\"Missing SERPAPI_API_KEY environment variable.\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "NUM_RESULTS_PER_QUERY = 20      # how many scholar results per query\n",
    "SLEEP_SECONDS = 2               # polite delay between requests\n",
    "LANG = \"en\"\n",
    "# If you want CN interface language, change to \"zh-CN\" but scholar content may vary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80bd12c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# -----------------------\n",
    "# 3) Helpers\n",
    "# -----------------------\n",
    "def stable_docid(title: str, link: str) -> str:\n",
    "    \"\"\"\n",
    "    Make a stable docid for pytrec_eval.\n",
    "    Prefer link+title hashing to avoid collisions.\n",
    "    \"\"\"\n",
    "    raw = (title or \"\").strip() + \"||\" + (link or \"\").strip()\n",
    "    h = hashlib.sha1(raw.encode(\"utf-8\")).hexdigest()\n",
    "    return f\"D{h[:16]}\"  # short but stable\n",
    "\n",
    "def rank_score(rank: int) -> float:\n",
    "    # Higher rank => higher score. Works for pytrec_eval ranking metrics.\n",
    "    return 1.0 / float(rank)\n",
    "\n",
    "def scholar_search(query: str, num: int) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Fetch results from SerpApi Google Scholar engine.\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        \"engine\": \"google_scholar\",\n",
    "        \"q\": query,\n",
    "        \"api_key\": API_KEY,\n",
    "        \"hl\": LANG,\n",
    "        \"num\": num,\n",
    "    }\n",
    "    search = GoogleSearch(params)\n",
    "    data = search.get_dict()\n",
    "    return data.get(\"organic_results\", [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fdad3ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad7a2550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Q001] Searching: numerical methods\n",
      "[Q002] Searching: finite element method\n",
      "[Q003] Searching: stochastic processes\n",
      "[Q004] Searching: optimization theory\n",
      "[Q005] Searching: statistical learning\n",
      "[Q006] Searching: panel data estimation\n",
      "[Q007] Searching: instrumental variables estimation\n",
      "[Q008] Searching: causal inference econometrics\n",
      "[Q009] Searching: nonparametric econometric methods\n",
      "[Q010] Searching: dynamic optimization in economics\n",
      "[Q011] Searching: general equilibrium theory\n",
      "[Q012] Searching: dynamic stochastic general equilibrium models\n",
      "[Q013] Searching: Hamilton–Jacobi–Bellman equation economics\n",
      "[Q014] Searching: viscosity solutions economic models\n",
      "[Q015] Searching: nonlinear dynamical systems in economics\n",
      "[Q016] Searching: Monte Carlo methods econometrics\n",
      "[Q017] Searching: numerical solution of DSGE models\n",
      "[Q018] Searching: semiparametric efficiency bound\n",
      "\n",
      "Done.\n",
      "Wrote: run.json, qrels_template.json, docs.json\n",
      "Next: fill qrels_template.json with relevance labels, then evaluate with pytrec_eval.\n",
      "Wrote: judgments.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# -----------------------\n",
    "# 4) Main: build run + docs + empty qrels template\n",
    "# -----------------------\n",
    "run: Dict[str, Dict[str, float]] = {}\n",
    "qrels_template: Dict[str, Dict[str, int]] = {}\n",
    "docs: Dict[str, Dict[str, Any]] = {}\n",
    "rows = []\n",
    "\n",
    "for i, q in enumerate(keywords, start=1):\n",
    "    qid = f\"Q{i:03d}\"\n",
    "    print(f\"[{qid}] Searching: {q}\")\n",
    "\n",
    "    results = scholar_search(q, NUM_RESULTS_PER_QUERY)\n",
    "\n",
    "    run[qid] = {}\n",
    "    qrels_template[qid] = {}\n",
    "\n",
    "    for r_idx, r in enumerate(results, start=1):\n",
    "        title = r.get(\"title\", \"\")\n",
    "        link = r.get(\"link\", \"\") or r.get(\"result_id\", \"\")  # fallback\n",
    "        snippet = r.get(\"snippet\", \"\")\n",
    "        publication_info = r.get(\"publication_info\", {})\n",
    "        cited_by = (r.get(\"inline_links\", {}).get(\"cited_by\", {}) or {}).get(\"total\", None)\n",
    "\n",
    "        docid = stable_docid(title, link)\n",
    "\n",
    "        # Store run score (rank-based)\n",
    "        run[qid][docid] = rank_score(r_idx)\n",
    "\n",
    "        # Store doc metadata for later inspection / judging\n",
    "        if docid not in docs:\n",
    "            docs[docid] = {\n",
    "                \"title\": title,\n",
    "                \"link\": link,\n",
    "                \"snippet\": snippet,\n",
    "                \"publication_info\": publication_info,\n",
    "                \"cited_by\": cited_by,\n",
    "            }\n",
    "        rows.append({\n",
    "        \"qid\": qid,\n",
    "        \"query\": q,\n",
    "        \"docid\": docid,\n",
    "        \"rank\": r_idx,\n",
    "        \"score\": rank_score(r_idx),\n",
    "        \"title\": title,\n",
    "        \"link\": link,\n",
    "        \"snippet\": snippet,\n",
    "        \"cited_by\": cited_by,\n",
    "        \"relevance\": 0   # 人工之后再改\n",
    "        })\n",
    "\n",
    "        # Empty qrels slot for human judgment (fill later with 0/1/2...)\n",
    "        # Example: qrels_template[qid][docid] = 1\n",
    "        qrels_template[qid][docid] = 0\n",
    "\n",
    "    time.sleep(SLEEP_SECONDS)\n",
    "\n",
    "# -----------------------\n",
    "# 5) Save JSON files\n",
    "# -----------------------\n",
    "with open(\"run.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(run, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "with open(\"qrels_template.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(qrels_template, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "with open(\"docs.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(docs, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"\\nDone.\")\n",
    "print(\"Wrote: run.json, qrels_template.json, docs.json\")\n",
    "print(\"Next: fill qrels_template.json with relevance labels, then evaluate with pytrec_eval.\")\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "df.to_csv(\n",
    "    \"judgments.csv\",\n",
    "    index=False,\n",
    "    encoding=\"utf-8\"\n",
    ")\n",
    "\n",
    "print(\"Wrote: judgments.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "134ccbb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run.json exists? True\n",
      "qrels_template.json exists? True\n",
      "docs.json exists? True\n",
      "queries in run: 18\n",
      "queries in qrels: 18\n",
      "docs: 360\n"
     ]
    }
   ],
   "source": [
    "import json, os\n",
    "\n",
    "for fn in [\"run.json\", \"qrels_template.json\", \"docs.json\"]:\n",
    "    print(fn, \"exists?\" , os.path.exists(fn))\n",
    "\n",
    "run = json.load(open(\"run.json\",\"r\",encoding=\"utf-8\"))\n",
    "qrels = json.load(open(\"qrels_template.json\",\"r\",encoding=\"utf-8\"))\n",
    "docs = json.load(open(\"docs.json\",\"r\",encoding=\"utf-8\"))\n",
    "\n",
    "print(\"queries in run:\", len(run))\n",
    "print(\"queries in qrels:\", len(qrels))\n",
    "print(\"docs:\", len(docs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f527b38a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min/avg/max results per query: 20 20.0 20\n",
      "empty qids: []\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "run = json.load(open(\"run.json\",\"r\",encoding=\"utf-8\"))\n",
    "\n",
    "counts = {qid: len(d) for qid, d in run.items()}\n",
    "print(\"min/avg/max results per query:\",\n",
    "      min(counts.values()),\n",
    "      sum(counts.values())/len(counts),\n",
    "      max(counts.values()))\n",
    "print(\"empty qids:\", [qid for qid,n in counts.items() if n==0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "536668b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SERPAPI_API_KEY loaded? True len= 64\n",
      "HTTP status: 200\n",
      "Top-level keys: ['search_metadata', 'search_parameters', 'search_information', 'organic_results', 'related_searches', 'pagination', 'serpapi_pagination']\n",
      "search_metadata: {'id': '6950cb3866bc780b189b0fa7', 'status': 'Success', 'json_endpoint': 'https://serpapi.com/searches/a19b7f07460ca09e/6950cb3866bc780b189b0fa7.json', 'created_at': '2025-12-28 06:16:24 UTC', 'processed_at': '2025-12-28 06:16:24 UTC', 'google_scholar_url': 'https://scholar.google.com/scholar?q=numerical+methods&hl=en&num=5', 'raw_html_file': 'https://serpapi.com/searches/a19b7f07460ca09e/6950cb3866bc780b189b0fa7.html', 'total_time_taken': 0.93}\n",
      "search_metadata.status: Success\n",
      "error: None\n",
      "error_message: None\n",
      "organic_results length: 5\n",
      "Saved full response to serpapi_debug.json\n"
     ]
    }
   ],
   "source": [
    "import os, requests, json\n",
    "\n",
    "API_KEY = os.getenv(\"SERPAPI_API_KEY\")\n",
    "print(\"SERPAPI_API_KEY loaded?\", API_KEY is not None, \"len=\", 0 if API_KEY is None else len(API_KEY))\n",
    "\n",
    "url = \"https://serpapi.com/search\"\n",
    "params = {\n",
    "    \"engine\": \"google_scholar\",\n",
    "    \"q\": \"numerical methods\",\n",
    "    \"num\": 5,\n",
    "    \"hl\": \"en\",\n",
    "    \"api_key\": API_KEY,\n",
    "}\n",
    "\n",
    "r = requests.get(url, params=params, timeout=30)\n",
    "print(\"HTTP status:\", r.status_code)\n",
    "\n",
    "try:\n",
    "    data = r.json()\n",
    "except Exception as e:\n",
    "    print(\"JSON decode error:\", e)\n",
    "    print(\"Raw text (first 500 chars):\", r.text[:500])\n",
    "    raise\n",
    "\n",
    "print(\"Top-level keys:\", list(data.keys()))\n",
    "print(\"search_metadata:\", data.get(\"search_metadata\"))\n",
    "print(\"search_metadata.status:\", (data.get(\"search_metadata\") or {}).get(\"status\"))\n",
    "print(\"error:\", data.get(\"error\"))\n",
    "print(\"error_message:\", data.get(\"error_message\"))\n",
    "print(\"organic_results length:\", len(data.get(\"organic_results\", [])))\n",
    "\n",
    "# 保存完整响应，便于你检查/发我\n",
    "with open(\"serpapi_debug.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(data, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Saved full response to serpapi_debug.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22aa8599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len= 32\n",
      "prefix= PAST suffix= HERE\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"SERPAPI_API_KEY\"] = \"PASTE_YOUR_FULL_SERPAPI_KEY_HERE\"\n",
    "\n",
    "k = os.getenv(\"SERPAPI_API_KEY\")\n",
    "print(\"len=\", len(k))\n",
    "print(\"prefix=\", k[:4], \"suffix=\", k[-4:])\n",
    "\n",
    "\n",
    "######## need to change api key ###############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5da55ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Q001] numerical methods -> got 10 results (target 10)\n",
      "Saved: run_scholar_k10.json, qrels_template_k10.json, docs_scholar_k10.json\n"
     ]
    }
   ],
   "source": [
    "import os, json, hashlib, time\n",
    "from serpapi import GoogleSearch\n",
    "\n",
    "API_KEY = os.getenv(\"SERPAPI_API_KEY\")\n",
    "assert API_KEY, \"Missing SERPAPI_API_KEY\"\n",
    "\n",
    "K = 10          # 与 AstA 对齐：每个 query 10 条\n",
    "HL = \"en\"       # Scholar 界面语言\n",
    "SLEEP_S = 1.0   # 请求间隔（可调）\n",
    "\n",
    "def stable_docid(title: str, link: str) -> str:\n",
    "    raw = (title or \"\").strip() + \"||\" + (link or \"\").strip()\n",
    "    h = hashlib.sha1(raw.encode(\"utf-8\")).hexdigest()\n",
    "    return f\"D{h[:16]}\"\n",
    "\n",
    "def rank_score(rank: int) -> float:\n",
    "    # pytrec_eval 只需要可排序的分数；Scholar 没有原生 score，用 rank-based 很标准\n",
    "    return 1.0 / rank\n",
    "\n",
    "def scholar_topk(query: str, k: int = K, hl: str = HL):\n",
    "    params = {\n",
    "        \"engine\": \"google_scholar\",\n",
    "        \"q\": query,\n",
    "        \"api_key\": API_KEY,\n",
    "        \"num\": k,      # 关键：直接取 10 条\n",
    "        \"start\": 0,    # 不翻页\n",
    "        \"hl\": hl,\n",
    "    }\n",
    "    data = GoogleSearch(params).get_dict()\n",
    "    return (data.get(\"organic_results\") or [])[:k]\n",
    "\n",
    "def build_pytrec_eval_json(queries, k: int = K):\n",
    "    run = {}\n",
    "    qrels_template = {}\n",
    "    docs = {}\n",
    "\n",
    "    for i, q in enumerate(queries, start=1):\n",
    "        qid = f\"Q{i:03d}\"\n",
    "        results = scholar_topk(q, k=k)\n",
    "\n",
    "        run[qid] = {}\n",
    "        qrels_template[qid] = {}\n",
    "\n",
    "        for rank, r in enumerate(results, start=1):\n",
    "            title = r.get(\"title\", \"\")\n",
    "            link = r.get(\"link\", \"\") or r.get(\"result_id\", \"\")\n",
    "            snippet = r.get(\"snippet\", \"\")\n",
    "            pub = r.get(\"publication_info\", {}) or {}\n",
    "            cited_by = (r.get(\"inline_links\", {}).get(\"cited_by\", {}) or {}).get(\"total\", None)\n",
    "\n",
    "            docid = stable_docid(title, link)\n",
    "\n",
    "            run[qid][docid] = rank_score(rank)\n",
    "\n",
    "            if docid not in docs:\n",
    "                docs[docid] = {\n",
    "                    \"title\": title,\n",
    "                    \"link\": link,\n",
    "                    \"snippet\": snippet,\n",
    "                    \"publication_info\": pub,\n",
    "                    \"cited_by\": cited_by,\n",
    "                }\n",
    "\n",
    "            # 默认 0；你后面人工改成 1/2/3...\n",
    "            qrels_template[qid][docid] = 1\n",
    "\n",
    "        print(f\"[{qid}] {q} -> got {len(results)} results (target {k})\")\n",
    "        time.sleep(SLEEP_S)\n",
    "    with open(\"run_scholar_k10.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(run, f, ensure_ascii=False, indent=2)\n",
    "    with open(\"qrels_template_k10.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(qrels_template, f, ensure_ascii=False, indent=2)\n",
    "    with open(\"docs_scholar_k10.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(docs, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(\"Saved: run_scholar_k10.json, qrels_template_k10.json, docs_scholar_k10.json\")\n",
    "\n",
    "# ---- 用你的 query list 替换这里（示例先放 1 个）----\n",
    "queries = [\"numerical methods\"]\n",
    "\n",
    "build_pytrec_eval_json(queries, k=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66363d7a",
   "metadata": {},
   "source": [
    "# Asta paper finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72974b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, hashlib\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "keywords = [\n",
    "    \"numerical methods\",\n",
    "    \"finite element method\",\n",
    "    \"stochastic processes\",\n",
    "    \"optimization theory\",\n",
    "    \"statistical learning\",\n",
    "    \"panel data estimation\",\n",
    "    \"instrumental variables estimation\",\n",
    "    \"causal inference econometrics\",\n",
    "    \"nonparametric econometric methods\",\n",
    "    \"dynamic optimization in economics\",\n",
    "    \"general equilibrium theory\",\n",
    "    \"dynamic stochastic general equilibrium models\",\n",
    "    \"Hamilton–Jacobi–Bellman equation economics\",\n",
    "    \"viscosity solutions economic models\",\n",
    "    \"nonlinear dynamical systems in economics\",\n",
    "    \"Monte Carlo methods econometrics\",\n",
    "    \"numerical solution of DSGE models\",\n",
    "    \"semiparametric efficiency bound\",\n",
    "]\n",
    "\n",
    "ASTA_MCP_URL = \"https://asta-tools.allen.ai/mcp/v1\"\n",
    "NUM_RESULTS_PER_QUERY = 50\n",
    "FIELDS = \"paperId,title,url,year,venue,authors,abstract,tldr,publicationDate,fieldsOfStudy,isOpenAccess,openAccessPdf\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d308d529",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rank_score(rank: int) -> float:\n",
    "    return 1.0 / float(rank)\n",
    "\n",
    "def stable_docid(paper_id: str, title: str = \"\", url: str = \"\") -> str:\n",
    "    if paper_id:\n",
    "        return \"A_\" + paper_id.replace(\":\", \"_\")\n",
    "    raw = (title or \"\").strip() + \"||\" + (url or \"\").strip()\n",
    "    h = hashlib.sha1(raw.encode(\"utf-8\")).hexdigest()\n",
    "    return f\"A_{h[:16]}\"\n",
    "\n",
    "def papers_from_calltool_result(resp) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Asta MCP: resp.content is a list of TextContent, each one is a JSON string for a paper.\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    content = getattr(resp, \"content\", None) or []\n",
    "    for item in content:\n",
    "        if getattr(item, \"type\", None) == \"text\":\n",
    "            txt = getattr(item, \"text\", \"\")\n",
    "            if not txt:\n",
    "                continue\n",
    "            try:\n",
    "                obj = json.loads(txt)\n",
    "                if isinstance(obj, dict) and (\"paperId\" in obj or \"title\" in obj):\n",
    "                    out.append(obj)\n",
    "            except Exception:\n",
    "                # 如果偶尔不是 JSON，就跳过\n",
    "                continue\n",
    "        elif getattr(item, \"type\", None) == \"json\":\n",
    "            obj = getattr(item, \"json\", None)\n",
    "            if isinstance(obj, dict):\n",
    "                out.append(obj)\n",
    "            elif isinstance(obj, list):\n",
    "                out.extend([x for x in obj if isinstance(x, dict)])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d2f34d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.11.14 (main, Oct 21 2025, 18:27:30) [Clang 20.1.8 ]\n",
      "mcp imported OK\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)\n",
    "import mcp\n",
    "print(\"mcp imported OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8794bbfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCP tools available: ['get_paper', 'get_paper_batch', 'get_citations', 'search_authors_by_name', 'get_author_papers', 'search_papers_by_relevance', 'search_paper_by_title', 'snippet_search']\n",
      "[Q001] searching: numerical methods\n",
      "[Q002] searching: finite element method\n",
      "[Q003] searching: stochastic processes\n",
      "[Q004] searching: optimization theory\n",
      "[Q005] searching: statistical learning\n",
      "[Q006] searching: panel data estimation\n",
      "[Q007] searching: instrumental variables estimation\n",
      "[Q008] searching: causal inference econometrics\n",
      "[Q009] searching: nonparametric econometric methods\n",
      "[Q010] searching: dynamic optimization in economics\n",
      "[Q011] searching: general equilibrium theory\n",
      "[Q012] searching: dynamic stochastic general equilibrium models\n",
      "[Q013] searching: Hamilton–Jacobi–Bellman equation economics\n",
      "[Q014] searching: viscosity solutions economic models\n",
      "[Q015] searching: nonlinear dynamical systems in economics\n",
      "[Q016] searching: Monte Carlo methods econometrics\n",
      "[Q017] searching: numerical solution of DSGE models\n",
      "[Q018] searching: semiparametric efficiency bound\n",
      "\n",
      "✅ Saved files: asta_run.json, asta_qrels_template.json, asta_docs.json\n",
      "docs count: 899\n",
      "min results/query: 50\n"
     ]
    }
   ],
   "source": [
    "async def run_asta():\n",
    "    key = os.getenv(\"ASTA_TOOL_KEY\")\n",
    "    if not key:\n",
    "        raise RuntimeError(\"Missing ASTA_TOOL_KEY in this kernel.\")\n",
    "\n",
    "    from mcp import ClientSession\n",
    "    from mcp.client.streamable_http import streamablehttp_client\n",
    "\n",
    "    headers = {\"x-api-key\": key}\n",
    "\n",
    "    run: Dict[str, Dict[str, float]] = {}\n",
    "    qrels_template: Dict[str, Dict[str, int]] = {}\n",
    "    docs: Dict[str, Dict[str, Any]] = {}\n",
    "\n",
    "    async with streamablehttp_client(ASTA_MCP_URL, headers=headers) as streams:\n",
    "        read, write = streams[0], streams[1]\n",
    "        async with ClientSession(read, write) as session:\n",
    "            tools = await session.list_tools()\n",
    "            tool_names = [t.name for t in tools.tools]\n",
    "            print(\"MCP tools available:\", tool_names)\n",
    "\n",
    "            if \"search_papers_by_relevance\" not in tool_names:\n",
    "                raise RuntimeError(f\"search_papers_by_relevance not found. Available: {tool_names}\")\n",
    "\n",
    "            for i, q in enumerate(keywords, start=1):\n",
    "                qid = f\"Q{i:03d}\"\n",
    "                print(f\"[{qid}] searching: {q}\")\n",
    "\n",
    "                resp = await session.call_tool(\n",
    "                    \"search_papers_by_relevance\",\n",
    "                    {\"keyword\": q, \"fields\": FIELDS, \"limit\": NUM_RESULTS_PER_QUERY},\n",
    "                )\n",
    "\n",
    "                papers = papers_from_calltool_result(resp)\n",
    "\n",
    "                run[qid] = {}\n",
    "                qrels_template[qid] = {}\n",
    "\n",
    "                for r_idx, p in enumerate(papers, start=1):\n",
    "                    paper_id = p.get(\"paperId\") or p.get(\"paper_id\") or p.get(\"id\") or \"\"\n",
    "                    title = p.get(\"title\", \"\")\n",
    "                    url = p.get(\"url\", \"\")\n",
    "\n",
    "                    docid = stable_docid(paper_id, title=title, url=url)\n",
    "                    run[qid][docid] = rank_score(r_idx)\n",
    "                    qrels_template[qid][docid] = 0\n",
    "\n",
    "                    if docid not in docs:\n",
    "                        docs[docid] = {\n",
    "                            \"paper_id\": paper_id,\n",
    "                            \"title\": title,\n",
    "                            \"url\": url,\n",
    "                            \"year\": p.get(\"year\"),\n",
    "                            \"venue\": p.get(\"venue\"),\n",
    "                            \"publicationDate\": p.get(\"publicationDate\"),\n",
    "                            \"authors\": p.get(\"authors\"),\n",
    "                            \"abstract\": p.get(\"abstract\"),\n",
    "                            \"tldr\": p.get(\"tldr\"),\n",
    "                            \"fieldsOfStudy\": p.get(\"fieldsOfStudy\"),\n",
    "                            \"isOpenAccess\": p.get(\"isOpenAccess\"),\n",
    "                            \"openAccessPdf\": p.get(\"openAccessPdf\"),\n",
    "                        }\n",
    "\n",
    "    with open(\"asta_run.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(run, f, ensure_ascii=False, indent=2)\n",
    "    with open(\"asta_qrels_template.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(qrels_template, f, ensure_ascii=False, indent=2)\n",
    "    with open(\"asta_docs.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(docs, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(\"\\n✅ Saved files: asta_run.json, asta_qrels_template.json, asta_docs.json\")\n",
    "    print(\"docs count:\", len(docs))\n",
    "    print(\"min results/query:\", min(len(v) for v in run.values()) if run else 0)\n",
    "\n",
    "# 在 Jupyter 里运行：\n",
    "await run_asta()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fbcb3137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "resp type: <class 'mcp.types.CallToolResult'>\n",
      "has content? True content len: 5\n",
      "\n",
      "--- content[0] type=text ---\n",
      "text head: {\n",
      "  \"paperId\": \"82482585e94192b4e9913727e461f89cd08e9725\",\n",
      "  \"url\": \"https://www.semanticscholar.org/paper/82482585e94192b4e9913727e461f89cd08e9725\",\n",
      "  \"title\": \"Pseudo Numerical Methods for Diffusion Models on Manifolds\",\n",
      "  \"year\": 2022,\n",
      "  \"openAccessPdf\": {\n",
      "    \"url\": \"\",\n",
      "    \"status\": null,\n",
      "    \"license\": null,\n",
      "    \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2202.09778, which is subject to the license by the author or copyright owner provided with this content.\n",
      "saved asta_debug_payload.json (from text)\n",
      "\n",
      "--- content[1] type=text ---\n",
      "text head: {\n",
      "  \"paperId\": \"8705e8fe0632bd11f200455a5125692a2547a018\",\n",
      "  \"url\": \"https://www.semanticscholar.org/paper/8705e8fe0632bd11f200455a5125692a2547a018\",\n",
      "  \"title\": \"Riemann Solvers and Numerical Methods for Fluid Dynamics\",\n",
      "  \"year\": 1997,\n",
      "  \"openAccessPdf\": {\n",
      "    \"url\": \"\",\n",
      "    \"status\": \"CLOSED\",\n",
      "    \"license\": null,\n",
      "    \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.1007/978-3-662-0\n",
      "saved asta_debug_payload.json (from text)\n",
      "\n",
      "--- content[2] type=text ---\n",
      "text head: {\n",
      "  \"paperId\": \"e1053197256c6c3c0631377ec23a3f7dc1cb4781\",\n",
      "  \"url\": \"https://www.semanticscholar.org/paper/e1053197256c6c3c0631377ec23a3f7dc1cb4781\",\n",
      "  \"title\": \"Numerical methods for unconstrained optimization and nonlinear equations\",\n",
      "  \"year\": 1983,\n",
      "  \"openAccessPdf\": {\n",
      "    \"url\": \"\",\n",
      "    \"status\": \"CLOSED\",\n",
      "    \"license\": null,\n",
      "    \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or abstract available at https://api.unpaywall.org/v2/10.\n",
      "saved asta_debug_payload.json (from text)\n",
      "\n",
      "--- content[3] type=text ---\n",
      "text head: {\n",
      "  \"paperId\": \"ca46c28ef834d57af1a0ce1d6393cca1c8395acb\",\n",
      "  \"url\": \"https://www.semanticscholar.org/paper/ca46c28ef834d57af1a0ce1d6393cca1c8395acb\",\n",
      "  \"title\": \"Numerical Methods for Conservation Laws: From Analysis to Algorithms\",\n",
      "  \"year\": 2017,\n",
      "  \"openAccessPdf\": {\n",
      "    \"url\": \"https://epubs.siam.org/doi/pdf/10.1137/1.9781611975109.fm\",\n",
      "    \"status\": \"BRONZE\",\n",
      "    \"license\": null,\n",
      "    \"disclaimer\": \"Notice: The following paper fields have been elided by the publisher: {'abstract'}. Paper or a\n",
      "saved asta_debug_payload.json (from text)\n",
      "\n",
      "--- content[4] type=text ---\n",
      "text head: {\n",
      "  \"paperId\": \"04471528ea465e7396588756db53af14b880837a\",\n",
      "  \"url\": \"https://www.semanticscholar.org/paper/04471528ea465e7396588756db53af14b880837a\",\n",
      "  \"title\": \"Robust Numerical Methods for Singularly Perturbed Differential Equations-Supplements\",\n",
      "  \"year\": 2022,\n",
      "  \"openAccessPdf\": {\n",
      "    \"url\": \"http://arxiv.org/pdf/2209.02994\",\n",
      "    \"status\": \"GREEN\",\n",
      "    \"license\": null,\n",
      "    \"disclaimer\": \"Notice: Paper or abstract available at https://arxiv.org/abs/2209.02994, which is subject to the license \n",
      "saved asta_debug_payload.json (from text)\n"
     ]
    }
   ],
   "source": [
    "import os, json\n",
    "\n",
    "ASTA_MCP_URL = \"https://asta-tools.allen.ai/mcp/v1\"\n",
    "\n",
    "async def debug_one():\n",
    "    from mcp import ClientSession\n",
    "    from mcp.client.streamable_http import streamablehttp_client\n",
    "\n",
    "    key = os.getenv(\"ASTA_TOOL_KEY\")\n",
    "    headers = {\"x-api-key\": key}\n",
    "\n",
    "    async with streamablehttp_client(ASTA_MCP_URL, headers=headers) as streams:\n",
    "        read, write = streams[0], streams[1]\n",
    "        async with ClientSession(read, write) as session:\n",
    "            resp = await session.call_tool(\n",
    "                \"search_papers_by_relevance\",\n",
    "                {\n",
    "                    \"keyword\": \"numerical methods\",\n",
    "                    \"fields\": \"paperId,title,url,year,abstract\",\n",
    "                    \"limit\": 5\n",
    "                },\n",
    "            )\n",
    "\n",
    "            print(\"resp type:\", type(resp))\n",
    "            content = getattr(resp, \"content\", None)\n",
    "            print(\"has content?\", content is not None, \"content len:\", (len(content) if content else None))\n",
    "\n",
    "            if content:\n",
    "                for i, item in enumerate(content):\n",
    "                    print(f\"\\n--- content[{i}] type={getattr(item,'type',None)} ---\")\n",
    "                    if getattr(item, \"type\", None) == \"json\":\n",
    "                        print(\"json keys:\", list(item.json.keys())[:30])\n",
    "                        with open(\"asta_debug_payload.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "                            json.dump(item.json, f, ensure_ascii=False, indent=2)\n",
    "                        print(\"saved asta_debug_payload.json\")\n",
    "                    elif getattr(item, \"type\", None) == \"text\":\n",
    "                        print(\"text head:\", item.text[:500])\n",
    "                        # 如果 text 里是 JSON，也保存下来\n",
    "                        try:\n",
    "                            j = json.loads(item.text)\n",
    "                            with open(\"asta_debug_payload.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "                                json.dump(j, f, ensure_ascii=False, indent=2)\n",
    "                            print(\"saved asta_debug_payload.json (from text)\")\n",
    "                        except Exception:\n",
    "                            pass\n",
    "\n",
    "await debug_one()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "161a809b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queries: 18\n",
      "docs: 899\n",
      "min results/query: 50\n",
      "sample Q001 top5: [('A_82482585e94192b4e9913727e461f89cd08e9725', 1.0), ('A_8705e8fe0632bd11f200455a5125692a2547a018', 0.5), ('A_e1053197256c6c3c0631377ec23a3f7dc1cb4781', 0.3333333333333333), ('A_ca46c28ef834d57af1a0ce1d6393cca1c8395acb', 0.25), ('A_04471528ea465e7396588756db53af14b880837a', 0.2)]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "run = json.load(open(\"asta_run.json\",\"r\",encoding=\"utf-8\"))\n",
    "docs = json.load(open(\"asta_docs.json\",\"r\",encoding=\"utf-8\"))\n",
    "\n",
    "print(\"queries:\", len(run))\n",
    "print(\"docs:\", len(docs))\n",
    "print(\"min results/query:\", min(len(v) for v in run.values()))\n",
    "print(\"sample Q001 top5:\", list(run[\"Q001\"].items())[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b7edc763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: asta_judgments_top10.csv\n"
     ]
    }
   ],
   "source": [
    "import json, csv\n",
    "\n",
    "TOPK = 10  # 每个query标注前10条，省时且足够比较排序能力\n",
    "\n",
    "run = json.load(open(\"asta_run.json\",\"r\",encoding=\"utf-8\"))\n",
    "docs = json.load(open(\"asta_docs.json\",\"r\",encoding=\"utf-8\"))\n",
    "\n",
    "rows = []\n",
    "for qid, ranking in run.items():\n",
    "    top = sorted(ranking.items(), key=lambda x: x[1], reverse=True)[:TOPK]\n",
    "    for rank, (docid, score) in enumerate(top, start=1):\n",
    "        d = docs.get(docid, {})\n",
    "        rows.append({\n",
    "            \"qid\": qid,\n",
    "            \"rank\": rank,\n",
    "            \"docid\": docid,\n",
    "            \"score\": score,\n",
    "            \"title\": d.get(\"title\",\"\"),\n",
    "            \"year\": d.get(\"year\",\"\"),\n",
    "            \"venue\": d.get(\"venue\",\"\"),\n",
    "            \"url\": d.get(\"url\",\"\"),\n",
    "            \"abstract\": (d.get(\"abstract\",\"\") or \"\")[:500],\n",
    "            \"relevance\": \"\"  # 你填 0/1/2\n",
    "        })\n",
    "\n",
    "with open(\"asta_judgments_top10.csv\", \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "    w = csv.DictWriter(f, fieldnames=list(rows[0].keys()))\n",
    "    w.writeheader()\n",
    "    w.writerows(rows)\n",
    "\n",
    "print(\"Saved: asta_judgments_top10.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7297aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: asta_qrels.json (for pytrec_eval)\n"
     ]
    }
   ],
   "source": [
    "import csv, json\n",
    "from collections import defaultdict\n",
    "\n",
    "qrels = defaultdict(dict)\n",
    "\n",
    "with open(\"/Users/terreturn/Desktop/python /Project_paper_finder/asta_judgments_top10_graded.csv\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for row in csv.DictReader(f):\n",
    "        rel = row[\"relevance\"].strip()\n",
    "        if rel == \"\":\n",
    "            continue\n",
    "        qrels[row[\"qid\"]][row[\"docid\"]] = int(rel)\n",
    "\n",
    "with open(\"asta_qrels.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(qrels, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Saved: asta_qrels.json (for pytrec_eval)\")\n",
    "\n",
    "\n",
    "### Asta json file done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "be85c10a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved: pooled_judgments_top10.csv\n",
      "Next: open it and fill missing relevance (mostly scholar-only docs).\n"
     ]
    }
   ],
   "source": [
    "import json, csv\n",
    "\n",
    "RUN_SCHOLAR = \"run_scholar_k10.json\"\n",
    "RUN_ASTA    = \"asta_run.json\"\n",
    "DOCS_SCHOLAR = \"docs_scholar_k10.json\"\n",
    "DOCS_ASTA    = \"asta_docs.json\"\n",
    "ASTA_QRELS   = \"asta_qrels.json\"\n",
    "\n",
    "OUT_POOL_CSV = \"pooled_judgments_top10.csv\"\n",
    "\n",
    "def load(p):\n",
    "    with open(p, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "run_s = load(RUN_SCHOLAR)\n",
    "run_a = load(RUN_ASTA)\n",
    "docs_s = load(DOCS_SCHOLAR)\n",
    "docs_a = load(DOCS_ASTA)\n",
    "asta_qrels = load(ASTA_QRELS)\n",
    "\n",
    "common_qids = sorted(set(run_s) & set(run_a))\n",
    "rows = []\n",
    "seen = set()\n",
    "\n",
    "for qid in common_qids:\n",
    "    pool = set(run_s[qid]) | set(run_a[qid])  # ≤ 20\n",
    "    for docid in pool:\n",
    "        if (qid, docid) in seen:\n",
    "            continue\n",
    "        seen.add((qid, docid))\n",
    "\n",
    "        meta = docs_s.get(docid) or docs_a.get(docid) or {}\n",
    "        source = []\n",
    "        if docid in run_s[qid]: source.append(\"scholar\")\n",
    "        if docid in run_a[qid]: source.append(\"asta\")\n",
    "\n",
    "        rel = \"\"\n",
    "        if qid in asta_qrels and docid in asta_qrels[qid]:\n",
    "            rel = asta_qrels[qid][docid]   # 自动填 AstA 的标注\n",
    "\n",
    "        rows.append({\n",
    "            \"qid\": qid,\n",
    "            \"docid\": docid,\n",
    "            \"source\": \"+\".join(source),\n",
    "            \"title\": meta.get(\"title\",\"\"),\n",
    "            \"url\": meta.get(\"url\", meta.get(\"link\",\"\")),\n",
    "            \"snippet\": meta.get(\"abstract\", meta.get(\"snippet\",\"\")),\n",
    "            \"relevance\": rel\n",
    "        })\n",
    "\n",
    "with open(OUT_POOL_CSV, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n",
    "    w = csv.DictWriter(\n",
    "        f,\n",
    "        fieldnames=[\"qid\",\"docid\",\"source\",\"title\",\"url\",\"snippet\",\"relevance\"]\n",
    "    )\n",
    "    w.writeheader()\n",
    "    w.writerows(rows)\n",
    "\n",
    "print(\"Saved:\", OUT_POOL_CSV)\n",
    "print(\"Next: open it and fill missing relevance (mostly scholar-only docs).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89440542",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b038355d",
   "metadata": {},
   "source": [
    "# Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e03b3d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pytrec_eval\n",
    "\n",
    "def load_json(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def mean_metric(per_query_scores, metric):\n",
    "    vals = [v[metric] for v in per_query_scores.values() if metric in v]\n",
    "    return sum(vals) / len(vals) if vals else float(\"nan\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "62bd561b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, pandas as pd\n",
    "\n",
    "asta = json.load(open(\"useful/asta_run_from_judgments.json\"))\n",
    "scholar = json.load(open(\"useful/scholar_run.json\"))\n",
    "\n",
    "rows = []\n",
    "\n",
    "TOPK = 10\n",
    "\n",
    "for qid in asta.keys():\n",
    "    # Asta\n",
    "    for rank, (docid, score) in enumerate(\n",
    "        sorted(asta[qid].items(), key=lambda x: -x[1])[:TOPK], start=1\n",
    "    ):\n",
    "        rows.append({\n",
    "            \"qid\": qid,\n",
    "            \"docid\": docid,\n",
    "            \"source\": \"asta\",\n",
    "            \"rank\": rank,\n",
    "            \"score\": score,\n",
    "            \"relevance\": \"\"\n",
    "        })\n",
    "\n",
    "    # Scholar\n",
    "    for rank, (docid, score) in enumerate(\n",
    "        sorted(scholar[qid].items(), key=lambda x: -x[1])[:TOPK], start=1\n",
    "    ):\n",
    "        rows.append({\n",
    "            \"qid\": qid,\n",
    "            \"docid\": docid,\n",
    "            \"source\": \"scholar\",\n",
    "            \"rank\": rank,\n",
    "            \"score\": score,\n",
    "            \"relevance\": \"\"\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(rows).drop_duplicates(subset=[\"qid\",\"docid\"])\n",
    "df.to_csv(\"qrels_pooling.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f09d7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, json\n",
    "\n",
    "df = pd.read_csv(\"useful/qrels_pooling 2.csv\")\n",
    "\n",
    "qrels = {}\n",
    "for qid, grp in df.groupby(\"qid\"):\n",
    "    qrels[qid] = {\n",
    "        row.docid: int(row.relevance)\n",
    "        for row in grp.itertuples()\n",
    "        if int(row.relevance) > 0\n",
    "    }\n",
    "\n",
    "json.dump(qrels, open(\"qrels_pooled.json\",\"w\"), indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "988c3c77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== AVERAGE METRICS ===\n",
      "map           ASTA=0.4891   SCHOLAR=0.4944\n",
      "ndcg_cut_10   ASTA=0.8911   SCHOLAR=0.9323\n",
      "recall_10     ASTA=0.4955   SCHOLAR=0.5045\n",
      "P_10          ASTA=0.9444   SCHOLAR=0.9611\n",
      "\n",
      "=== PER-QUERY NDCG@10 DIFF (ASTA - SCHOLAR) ===\n",
      "Q001: -0.0392\n",
      "Q002: +0.0000\n",
      "Q003: +0.0000\n",
      "Q004: -0.0331\n",
      "Q005: +0.0000\n",
      "Q006: +0.0000\n",
      "Q007: +0.0694\n",
      "Q008: +0.1184\n",
      "Q009: +0.2393\n",
      "Q010: +0.0000\n",
      "Q011: +0.0000\n",
      "Q012: -0.0318\n",
      "Q013: -0.4485\n",
      "Q014: -0.2755\n",
      "Q015: -0.0298\n",
      "Q016: -0.1420\n",
      "Q017: +0.0464\n",
      "Q018: -0.2151\n"
     ]
    }
   ],
   "source": [
    "# 1) Load\n",
    "qrels   = load_json(\"useful/qrels_pooled.json\")\n",
    "asta    = load_json(\"useful/asta_run_from_judgments.json\")\n",
    "scholar = load_json(\"useful/scholar_run.json\")\n",
    "\n",
    "# 2) Evaluate\n",
    "metrics = {\"map\", \"ndcg_cut_10\", \"recall_10\", \"P_10\"}\n",
    "evaluator = pytrec_eval.RelevanceEvaluator(qrels, metrics)\n",
    "\n",
    "asta_per_q = evaluator.evaluate(asta)\n",
    "scholar_per_q = evaluator.evaluate(scholar)\n",
    "\n",
    "# 3) Print summary\n",
    "print(\"=== AVERAGE METRICS ===\")\n",
    "for m in [\"map\", \"ndcg_cut_10\", \"recall_10\", \"P_10\"]:\n",
    "    print(f\"{m:12s}  ASTA={mean_metric(asta_per_q, m):.4f}   SCHOLAR={mean_metric(scholar_per_q, m):.4f}\")\n",
    "\n",
    "# 4) (Optional) per-query diffs\n",
    "print(\"\\n=== PER-QUERY NDCG@10 DIFF (ASTA - SCHOLAR) ===\")\n",
    "for qid in sorted(qrels.keys()):\n",
    "    a = asta_per_q.get(qid, {}).get(\"ndcg_cut_10\", float(\"nan\"))\n",
    "    s = scholar_per_q.get(qid, {}).get(\"ndcg_cut_10\", float(\"nan\"))\n",
    "    print(f\"{qid}: {a - s:+.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
